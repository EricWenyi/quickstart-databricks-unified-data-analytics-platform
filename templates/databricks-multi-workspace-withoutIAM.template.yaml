AWSTemplateFormatVersion: 2010-09-09
Description: >-
  This template creates resources for a Databricks workspace in your AWS account using the Account API. The Account API is required if you want to use the following optional features: customer-managed VPCs and customer-managed keys for notebooks. Contact your Databricks representative to determine the availability of these features for your subscription and deployment type. (qs-1r0odiedc)
  
Metadata:
  QuickStartDocumentation:
    EntrypointName: "Parameters for deploying a workspace and using an existing cross-account IAM role"
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Workspace configuration"
        Parameters:
          - TagValue
          - AccountId
          - AWSAccountId 
          - AWSRegion
          - PricingTier
          - DeploymentName
          - Username
          - Password
      - Label:
          default: "Required IAM role and S3 bucket configuration"  
        Parameters:
          - IAMArn
          - IAMArnLambda
          - BucketName    
      - Label:
          default: "(Optional) Customer-managed-key configuration for notebooks—requires the Enterprise tier"
        Parameters:
          - KeyArn
          - KeyAlias  
          - KeyRegion
      - Label:
          default: "(Optional) Customer-managed VPC configuration—requires the Premium tier"
        Parameters:
          - VPCID
          - SubnetIDs
          - SecurityGroupIDs
      - Label:
          default: "Quick Start configuration"
        Parameters:
          - QSS3BucketName
          - QSS3KeyPrefix
    ParameterLabels:
      AccountId:
        default: Databricks account ID
      AWSAccountId:
        default: AWS account ID
      AWSRegion:
        default: AWS Region of the Databricks workspace
      PricingTier:
        default: Pricing tier of the workspace
      DeploymentName:
        default: New workspace deployment name
      Username:
        default: Workspace account email address
      Password:
        default: Workspace account password
      IAMArn:
        default: ARN of the existing cross-account IAM role
      IAMArnLambda:
        default: ARN of the existing IAM role with Lambda execution and S3 access permissions
      BucketName:
        default: Root S3 bucket name  
      KeyArn:
        default: ARN for customer-managed AWS KMS key
      KeyAlias:
        default: Alias name for customer-managed AWS KMS key
      KeyRegion:
        default: AWS Region for customer-managed AWS KMS key
      VPCID:
        default: VPC ID
      SubnetIDs:
        default: Private-subnet IDs
      SecurityGroupIDs:
        default: Security-group IDs
      QSS3BucketName:
        default: Quick Start S3 bucket name
      QSS3KeyPrefix:
        default: Quick Start S3 key prefix
      TagValue:
        default: Tag value used for all AWS resources

Outputs:
  S3BucketName:
    Description: Name of the S3 root bucket
    Value: !Ref assetsS3Bucket
  CustomerManagedKeyId:
    Description: ID of the created customer-managed key object  
    Condition: IsKMSKeyProvided
    Value: !GetAtt createCustomerManagedKey.CustomerManagedKeyId
  CredentialsId:
    Description: Credential ID
    Value: !GetAtt createCredentials.CredentialsId
  ExternalId:
    Description: Databricks external ID
    Value: !GetAtt createCredentials.ExternalId
  NetworkId:
    Description: Databricks network ID
    Condition: CustomerManagedVPC
    Value: !GetAtt createNetworks.NetworkId 
  StorageConfigId:
    Description: Storage configuration ID
    Value: !GetAtt createStorageConfiguration.StorageConfigId
  WorkspaceURL:
    Description: URL of the workspace
    Value: !Join
      - ''
      - - 'https://'
        - !GetAtt createWorkspace.DeploymentName
        - '.cloud.databricks.com'
  WorkspaceStatus:
    Description: Status of the requested workspace
    Value: !GetAtt createWorkspace.WorkspaceStatus
  WorkspaceStatusMessage:
    Description: Detailed status description of the requested workspace
    Value: !GetAtt createWorkspace.WorkspaceStatusMsg
  PricingTier:
    Description: The pricing tier of the workspace. See https://databricks.com/product/aws-pricing.
    Value: !GetAtt createWorkspace.PricingTier

Parameters:
  AccountId:
    Description: "Your account must be on the E2 version of the platform. See https://docs.databricks.com/getting-started/overview.html#e2-architecture."
    MinLength: '32'
    Type: String
    Default: aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee 
  AWSAccountId:
    Description: "This must be a 12-digit number."
    AllowedPattern: '^\d{12}$'
    ConstraintDescription: Must be a valid 12-digit number
    MinLength: '12'
    Type: String
    Default: 111111111111
  AWSRegion:
    Description: "AWS Region where the workspace will be created. Customer-managed keys to encrypt notebooks are not supported in Region us-west-1."
    AllowedValues:
       - ca-central-1
       - eu-central-1
       - eu-west-1
       - us-east-1
       - us-east-2
       - us-west-1
       - us-west-2
    Type: String
  PricingTier:
    Description: "Choose a pricing tier. If you keep this box blank, the API will default to the highest tier available to your account. See https://databricks.com/product/aws-pricing."
    AllowedValues:
       - STANDARD
       - PREMIUM 
       - ENTERPRISE
       - '' 
    Type: String   
    Default: ''  
  DeploymentName:
    Description: "(Optional but recommended) Unique name that defines part of the workspace subdomain, such as <workspace-deployment-name>.cloud.databricks.com. Hyphens (-) are allowed but not as the first or last character. If your account has a deployment name prefix, the prefix (followed by a hyphen) is added before the deployment name. If your account has a deployment name prefix and you set DeploymentName to the reserved keyword EMPTY, DeploymentName is replaced by your account prefix. See https://docs.databricks.com/administration-guide/account-api/new-workspace.html#step-5-create-the-workspace."
    AllowedPattern: '^(([a-z0-9][a-z0-9-]*[a-z0-9])|([a-z0-9]))|(EMPTY)$'
    ConstraintDescription: You must specify a valid subdomain for the workspace or the reserved word EMPTY.
    Type: String 
  Username:
    Description: "Your account email address, which is used for REST API authentication as your user name. This is case-sensitive. Use the same capitalization as when you sent it to your Databricks representative."
    AllowedPattern: '^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+$'
    ConstraintDescription: Must be a valid email format.
    MinLength: '8'
    Type: String
  Password:
    Description: "Your account password, which is used for REST API authentication. This is case-sensitive. Minimum length is 8 characters."
    MinLength: '8'
    NoEcho: 'true'
    Type: String
  IAMArn: 
    Description: "Specify an existing IAM role ARN. For instructions on creating an IAM role, see https://docs.databricks.com/administration-guide/multiworkspace/iam-role.html."
    Type: String
    AllowedPattern: 'arn:aws:iam::\d{12}:role/.*'
    ConstraintDescription: Must be an IAM role ARN.
    MinLength: 16
    Default: 'arn:aws:iam::111111111111:role/your-role-name'
  IAMArnLambda: 
    Description: "Specify an existing IAM role ARN with AWSLambdaBasicExecutionRole. See the deployment guide for guidance."
    Type: String
    AllowedPattern: 'arn:aws:iam::\d{12}:role/.*'
    ConstraintDescription: Must be an IAM role ARN.
    MinLength: 16   
    Default: 'arn:aws:iam::111111111111:role/your-role-name'
  BucketName:
    Description: "Name of your new S3 root bucket. Only alphanumeric characters. For naming rules, https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html."
    AllowedPattern: '(?=^.{3,63}$)(?!xn--)([a-z0-9](?:[a-z0-9-]*)[a-z0-9])$'
    MinLength: '3'
    MaxLength: '63'
    Type: String  
    ConstraintDescription: The Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-).
  KeyArn:
    Description: "(Optional) AWS KMS key ARN to encrypt and decrypt the workspace notebooks in the control plane. Set only if using the feature customer-managed key for notebooks. See https://docs.databricks.com/security/keys/customer-managed-keys-notebook-aws.html."
    Default: ''
    Type: String
  KeyAlias:
    Description: "(Optional) AWS KMS key alias. Keep blank if you didn't specify a key ARN."
    Default: ''
    Type: String 
  KeyRegion:
    Description: "(Optional) AWS Region of your AWS KMS key. Keep blank if you didn't specify a key ARN."
    Default: ''
    Type: String
  VPCID:
    Description: "(Optional) ID for your VPC in which to create the new workspace. Set only if using the customer-managed VPC feature. Format is vpc-xxxxxxxxxxxxxxxx. See https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html. If unspecified, Databricks creates the new workspace in a new VPC that Databricks creates."
    Type: String  
    Default: ''  
  SecurityGroupIDs:
    Description: "(Optional) Names of one or more security groups in your VPC. The format is sg-xxxxxxxxxxxxxxxxx. To provide multiple IDs, separate with commas. Databricks must have access to at least one security group and no more than five security groups. You can reuse existing security groups rather than create new ones. Keep blank if you didn't specify VPCID. See https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html."
    Type: String
    Default: ''
  SubnetIDs:
    Description: "(Optional) At least two private-subnet IDs in your VPC, separated by commas. These subnets cannot be shared with other workspaces nor any other non-Databricks resources. Each subnet must have a netmask between /17 and /25. Subnets must be private. Subnets must have outbound access to the public network using a NAT gateway and internet gateway or similar customer-managed appliance infrastructure. The NAT gateway must be set up in its own subnet that routes quad-zero (0.0.0.0/0) traffic to an internet gateway or similar customer-managed appliance infrastructure. Keep blank if you didn't specify VPCID. See https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html."
    Type: String 
    Default: '' 
  QSS3BucketName:
    Description: "S3 bucket that you created for your copy of Quick Start assets. Use this if you decide to customize the Quick Start. This bucket name can include numbers, lowercase letters, uppercase letters, and hyphens, but do not start or end with a hyphen (-)."
    AllowedPattern: '(?=^.{3,63}$)(?!xn--)([a-z0-9](?:[a-z0-9-]*)[a-z0-9])$'
    Default: aws-quickstart
    Type: String
    ConstraintDescription: 'The Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-).'
  QSS3KeyPrefix:
    Description: "S3 key prefix that is used to simulate a folder for your copy of Quick Start assets. Use this if you decide to customize the Quick Start. This prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slashes (/). See https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html."
    ConstraintDescription: 'The Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slashes (/).'
    AllowedPattern: '^[0-9a-zA-Z-/]*$'
    Default: quickstart-databricks-unified-data-analytics-platform/
    Type: String
  TagValue:
    Description: "All new AWS objects get a tag with the key name. Enter a value for this tag so that you can identify all new AWS objects that this template creates. See https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html."
    MinLength: '1'
    Type: String
    Default: databricks-quickstart-cloud-formation
  

Conditions:
# Condition to check if a VPC ID is provided by the user
  CustomerManagedVPC: !Not [!Equals [!Ref VPCID, '']]
# Condition to check if an AWS KMS key ID is provided by the user
  IsKMSKeyProvided: !Not [!Equals [!Ref KeyArn, '']]  

Rules:
  
# Ensure that the SubnetIds and SecurityGroupIds are provided, if the user provides a customer-managed VPC ID  
  CustomerManagedVPC:
    RuleCondition: !Not [!Equals [!Ref VPCID, '']]
    Assertions:
    - Assert: !Not [!Equals ['', !Ref SubnetIDs]]
      AssertDescription: SubnetIDs is required when VPCID is provided
    - Assert: !Not [!Equals ['', !Ref SecurityGroupIDs]]
      AssertDescription: SecurityGroupIDs is required when VPCID is provided
      
# Ensure that the KeyAlias and KeyRegion are provided, if user provides a customer-managed KMS   
  CustomerManagedKMS:
    RuleCondition: !Not [!Equals [!Ref KeyArn, '']]
    Assertions:    
    - Assert: !Not [!Equals ['', !Ref KeyAlias ]]
      AssertDescription: KeyAlias is required when the KeyArn is provided
    - Assert: !Not [!Equals ['', !Ref KeyRegion ]]
      AssertDescription: KeyRegion is required when the KeyArn is provided

# Assertion rule to prevent changing the QuickStart Bucket name and Prefix parameter
# This rule must be COMMENTED if it is intended to clone the git repo to make modifications prior to promote the changes

  AWSQuickStartGitParametersSettings:
    Assertions:    
    - Assert: !Equals ['aws-quickstart', !Ref QSS3BucketName]
      AssertDescription: The QSS3BucketName MUST be set to aws-quickstart  
    - Assert: !Equals ['quickstart-databricks-unified-data-analytics-platform/', !Ref QSS3KeyPrefix]
      AssertDescription: The QSS3KeyPrefix MUST be set to - quickstart-databricks-unified-data-analytics-platform/
        
Resources:
  
  # S3 root bucket requirements
  assetsS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref BucketName
  bucketPolicy:
    Type: 'AWS::S3::BucketPolicy'
    Properties:
      PolicyDocument:
        Id: MyPolicy
        Version: 2012-10-17
        Statement:
          - Sid: PreventAccidentalDelete
            Effect: Deny 
            Action: 
              - 's3:DeleteBucket'
            Resource:
              - !Sub 'arn:aws:s3:::${assetsS3Bucket}'
          - Sid: Grant Databricks Access
            Effect: Allow
            Principal:
              AWS: arn:aws:iam::414351767826:root
            Action:
              - 's3:GetObject'
              - 's3:GetObjectVersion'
              - 's3:PutObject'
              - 's3:DeleteObject'
              - 's3:ListBucket'
              - 's3:GetBucketLocation'
            Resource:
              - !Sub 'arn:aws:s3:::${assetsS3Bucket}/*'
              - !Sub 'arn:aws:s3:::${assetsS3Bucket}'
      Bucket: !Ref assetsS3Bucket                  

  # Databricks API for configuring notebook encryption with a customer-managed KMS, if provided
  createCustomerManagedKey:
    Condition: IsKMSKeyProvided
    Type: Custom::CreateCustomerManagedKey
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_CUSTOMER_MANAGED_KEY'
      accountId: !Ref AccountId
      key_arn: !Ref KeyArn
      key_alias: !Ref KeyAlias
      key_region: !Ref KeyRegion
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'

  # Databricks API for workspace credentials
  createCredentials:
    Type: Custom::CreateCredentials
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_CREDENTIALS'
      accountId: !Ref AccountId
      credentials_name: !Join
        - '-'
        - - !Ref DeploymentName
          - 'credentials'
      role_arn: !Ref 'IAMArn'
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'

  # Databricks API for workspace storage configuration    
  createStorageConfiguration:
    Type: Custom::CreateStorageConfigurations
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_STORAGE_CONFIGURATIONS'
      accountId: !Ref AccountId
      storage_config_name: !Join
        - '-'
        - - !Ref 'DeploymentName'
          - 'storage_configuration'
      s3bucket_name: !Ref assetsS3Bucket
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'

  # Databricks API for customer-managed VPC    
  createNetworks:
    DependsOn: createStorageConfiguration
    Condition: CustomerManagedVPC
    Type: Custom::createNetworks
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_NETWORKS'
      accountId: !Ref AccountId
      network_name: !Join
        - '-'
        - - !Ref 'DeploymentName'
          - 'network'
      vpc_id: !Ref VPCID
      subnet_ids: !Ref SubnetIDs
      security_group_ids: !Ref SecurityGroupIDs
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'

  # Databricks API for workspace creation    
  createWorkspace:
    Type: Custom::CreateWorkspace
    Properties:
      ServiceToken: !GetAtt 'databricksApiFunction.Arn'
      action: 'CREATE_WORKSPACES'
      accountId: !Ref AccountId
      workspace_name: !Join
        - '-'
        - - !Ref 'DeploymentName'
          - 'workspace'
      deployment_name: !Ref 'DeploymentName'
      aws_region: !Ref AWSRegion
      credentials_id: !GetAtt createCredentials.CredentialsId
      storage_config_id: !GetAtt createStorageConfiguration.StorageConfigId
      encodedbase64: 
        Fn::Base64: !Join
          - ':'
          - - !Ref 'Username'
            - !Ref 'Password'
      network_id: !If [CustomerManagedVPC, !GetAtt 'createNetworks.NetworkId', '']
      customer_managed_key_id: !If [IsKMSKeyProvided,!GetAtt 'createCustomerManagedKey.CustomerManagedKeyId', '']
      pricing_tier: !Ref PricingTier
      

  databricksApiFunction:
    DependsOn: CopyZips
    Type: AWS::Lambda::Function
    Properties:
      Description: Databricks Account API 
      Handler: rest_client.handler
      Runtime: python3.8
      Role: !Ref 'IAMArnLambda'
      Timeout: 900
      Code:
        S3Bucket: !Ref 'LambdaZipsBucket'
        S3Key: !Sub '${QSS3KeyPrefix}functions/packages/lambda.zip'   

  # Resources to stage lambda.zip file
  LambdaZipsBucket:
    Type: AWS::S3::Bucket
  CopyZips:
    Type: Custom::CopyZips
    Properties:
      ServiceToken: !GetAtt 'CopyZipsFunction.Arn'
      DestBucket: !Ref 'LambdaZipsBucket'
      SourceBucket: !Ref 'QSS3BucketName'
      Prefix: !Ref 'QSS3KeyPrefix'
      Objects:
        - functions/packages/lambda.zip

  CopyZipsFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Copies objects from a source S3 bucket to a destination
      Handler: index.handler
      Runtime: python2.7
      Role: !Ref 'IAMArnLambda'
      Timeout: 240
      Code:
        ZipFile: |
          import json
          import logging
          import threading
          import boto3
          import cfnresponse
          def copy_objects(source_bucket, dest_bucket, prefix, objects):
              s3 = boto3.client('s3')
              for o in objects:
                  key = prefix + o
                  copy_source = {
                      'Bucket': source_bucket,
                      'Key': key
                  }
                  print('copy_source: %s' % copy_source)
                  print('dest_bucket = %s'%dest_bucket)
                  print('key = %s' %key)
                  s3.copy_object(CopySource=copy_source, Bucket=dest_bucket,
                        Key=key)
          def delete_objects(bucket, prefix, objects):
              s3 = boto3.client('s3')
              objects = {'Objects': [{'Key': prefix + o} for o in objects]}
              s3.delete_objects(Bucket=bucket, Delete=objects)
          def timeout(event, context):
              logging.error('Execution is about to time out, sending failure response to CloudFormation')
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None)
          def handler(event, context):
              # make sure we send a failure to CloudFormation if the function
              # is going to timeout
              timer = threading.Timer((context.get_remaining_time_in_millis()
                        / 1000.00) - 0.5, timeout, args=[event, context])
              timer.start()
              print('Received event: %s' % json.dumps(event))
              status = cfnresponse.SUCCESS
              try:
                  source_bucket = event['ResourceProperties']['SourceBucket']
                  dest_bucket = event['ResourceProperties']['DestBucket']
                  prefix = event['ResourceProperties']['Prefix']
                  objects = event['ResourceProperties']['Objects']
                  if event['RequestType'] == 'Delete':
                      delete_objects(dest_bucket, prefix, objects)
                  else:
                      copy_objects(source_bucket, dest_bucket, prefix, objects)
              except Exception as e:
                  logging.error('Exception: %s' % e, exc_info=True)
                  status = cfnresponse.FAILED
              finally:
                  timer.cancel()
                  cfnresponse.send(event, context, status, {}, None)
                  